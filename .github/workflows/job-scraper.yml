name: Job Scraper Scheduler

on:
  schedule:
    # Run every day at 8:00 AM UTC (10:00 AM CET)
    - cron: "0 8 * * *"
  # Allow manual triggering
  workflow_dispatch:

jobs:
  scrape-and-send:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create logs directory
        run: mkdir -p logs

      - name: Create .env file from secrets
        run: |
          echo "FROM_EMAIL=${{ secrets.FROM_EMAIL }}" >> .env
          echo "PASSWORD=${{ secrets.EMAIL_PASSWORD }}" >> .env
          echo "TO_EMAILS=${{ secrets.TO_EMAILS }}" >> .env
          echo "SMTP_SERVER=${{ secrets.SMTP_SERVER }}" >> .env
          echo "SMTP_PORT=${{ secrets.SMTP_PORT }}" >> .env
          echo "DEFAULT_SUBJECT=${{ secrets.DEFAULT_SUBJECT }}" >> .env

      - name: Run job scraper
        run: python main.py

      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: scraper-logs-${{ github.run_number }}
          path: logs/
          retention-days: 30
